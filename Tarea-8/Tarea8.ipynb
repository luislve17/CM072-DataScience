{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 8 del curso CM-072\n",
    "\n",
    "* Nombre y apellidos: Luis Vasquez Espinoza\n",
    "* Fecha de presentación: 30 de noviembre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responde las siguientes preguntas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es la diferencia entre la Inteligencia Artificial Fuerte y la Inteligencia Artificial Débil?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:** \n",
    "Una IA fuerte es aquella que se entiende lo suficiente a sí misma como para superarse y evolucionar autónomamente.\n",
    "Aún si no es equivalente a un humano (filosóficamente hablando) o realizar todas las tareas que nosotros realizamos naturalmente, esta IA puede generar una enorme cantidad de poder de optimización, buena toma de decisiones y su creación sería de importancia histórica.\n",
    "\n",
    "Una IA débil, por el contrario, es una IA sin capacidad limitada para auto modificarse. Un chessbot que se ejecuta en su computadora portátil puede tener una capacidad sobrehumana para jugar al ajedrez, pero solo puede jugar al ajedrez, y aunque puede ajustar sus pesos o su arquitectura y mejorar lentamente, no puede modificarse de una manera lo suficientemente profunda como para generalizar a otras tareas. .\n",
    "\n",
    "Otra forma de pensar sobre esto es que una IA fuerte es un investigador de IA por derecho propio, y una IA débil es lo que producen los investigadores de AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es un sistema experto y cuales son las características más importantes de un sistema experto? Listar las ventajas del sistema experto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Los sistemas expertos son sistemas computacionales que personifican algunas habilidades no algorítmicas para resolver ciertos tipos de problemas con la habilidad de un humano experto. Para que un sistema sea categorizado como *experto* debe presentar las siguientes características:\n",
    "\n",
    "* 1. Especificidad de dominio\n",
    "* 2. Lenguaje de programación especial (LISP, PROLOG)\n",
    "* 3. Operatividad en un sistema interactivo\n",
    "* 4. Mecanismo de filtrado, expansión y actualización de la información\n",
    "* 5. Capacidad de hacer inferencias lógicas a partir del conocimiento almacenado\n",
    "* 6. Habilidad de explicar razonamiento.\n",
    "* 7. Capacidad de asignación de intervalos de confianza\n",
    "* 8. Ser una alternativa efectiva a un experto humano (en función al costo)\n",
    "\n",
    "**Beneficios:**\n",
    "* Aumento de la probabilidad, frecuencia y consistencia de toma de decisiones.\n",
    "* Ayuda a expandir la habilidad humana.\n",
    "* Facilita la resolución de problemas de manera eficiente, por no-expertos.\n",
    "* Optimiza la manipulación de data\n",
    "* Permite el estudio imparcial de área de trabajo, evitando el involucrar la opinión y los sentimientos.\n",
    "* Permite dinamismo durante la maduración del sistema.\n",
    "* Habilita a los expertos a invertir su tiempo en actividades mas creativas\n",
    "* Inspira a investigadores a estudiar las ramas del área de trabajo original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es Tensorflow y Keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Ambas son librerías dedicadas a la programación de inteligencia artificial, especializándose en redes neuronales.\n",
    "La primera, Tensorflow, ofrece un catálogo robusto de métodos y clases de bajo nivel para implementar la topología de las redes, mientras que Keras ofrece un entorno de trabajo de más alto nivel para construir redes mas fácilmente pero con poco margen de modificación (almenos no tanto como Tensorflow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumera las diferentes técnicas de algoritmos en Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta.**\n",
    "* 1. Regresión\n",
    "* 2. Clasificación\n",
    "* 3. Clusterización\n",
    "* 4. Detección de anomalías (Análisis asociativo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Una manera de modelar un sistema basado en datos es entrenando una red neuronal artificial para que generalice sus tendencias a partir de sus atributos. Cuando esta topología de red presenta más de una capa oculta (o interna) hablamos de **Deep learning**, rubro de estudio de las RN comúnmente usado para trabajos más complejos (ej. procesamiento de imágenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las diferencias entre aprendizaje supervisado, no supervisado y por refuerzo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Supervisado:** Se conoce tanto el input como el output esperado, y es a partir de estos datos que se busca formar un modelo que los generalice.\n",
    "\n",
    "**2. No supervisado:** Solo se conoce el input y es el sistema el que generaliza a su manera el output. No existe una respuesta correcta real y nos sostenemos de medidas de tendencia central para construir el modelo con la suficiente confianza\n",
    "\n",
    "**3. Por refuerzo:** Independientemente del hecho de saber si el output es conocido *a priori* o no, este método puntúa el proceso de aprendizaje con estímulos o castigos al sistema iterativamente, en función a sus resultados parciales durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombra algunos algoritmos de machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "- Logistic Regression\n",
    "- K Nearest Neighbor\n",
    "- Naïve Bayes\n",
    "- K Means\n",
    "- Classification and Regression Trees\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "\n",
    "Algoritmo de conteo estadístico basado en la regla de Bayes y asumiendo que la probabilidad de clasificación de las muestras es independiente. Se aplica la regla a la data para predecir la clase a la que pertenece evalúando las probabilidades asociadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son diferencias entre  modelos paramétricos y no paramétricos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Los modelos paramétricos suponen un conjunto finito de parámetros. Dados los parámetros, las predicciones futuras, x, son independientes de los datos observados, por lo tanto se captura todo lo que hay que saber sobre los datos. Por lo tanto, la complejidad del modelo está limitada incluso si la cantidad de datos es ilimitado. Esto los hace no muy flexibles.\n",
    "\n",
    "Los modelos no paramétricos suponen que la distribución de datos no se puede definir en términos de un conjunto finito de parámetros. Pero a menudo se pueden definir asumiendo una dimensión infinita. Por lo general, pensamos en estas dimensiones como funciones. La cantidad de información que se puede capturar sobre los datos puede crecer a medida que la cantidad de datos crece. Esto los hace más flexibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista las técnicas de extracción usados para la reducción de la dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta**\n",
    "* Principal component analysis (PCA)\n",
    "* Non-negative matrix factorization (NMF)\n",
    "* Linear discriminant analysis (LDA)\n",
    "* Generalized discriminant analysis (GDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿ Es kNN diferente de kmeans clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Son cosas totalmente diferentes, lo único similar es que terminan agrupando elementos en clases pero eso solo los pondría en la misma categoría de \"clasificadores\".\n",
    "KNN es un algoritmo supervisado, de alta demanda computacional y dependiente del meta-parámetro **k**; mientras que k-means es no-supervisado, se basa en la media como medida de tendencia central regularizadora y busca generar clústers a partir de centroides que hacen las de representantes de las clases involucradas (y desconocidas al principio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista los pasos involucrados en machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "- Recolección de datos\n",
    "- Preprocesamiento de datos\n",
    "- Exploraación de datos\n",
    "- Entrenamiento del modelo\n",
    "- Evaluar el modelo\n",
    "- Utilizar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es la regularización, por qué la usamos y proporciona algunos ejemplos de métodos comunes en machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta**\n",
    "La regularización es un método regresivo que es muchas veces considerado como un modelo de regresión independiente. Esta forma restringe (o regulariza) las estimaciones de los coeficientes a ajustar hacia el valor cero; así el modelo penaliza los ajustes más complejos o flexibles, para evitar la sobrealimentación.\n",
    "\n",
    "Una relación simple para la regresión lineal se ve así. Aquí $Y$ representa la relación aprendida y $\\beta$ representa las estimaciones de coeficientes para diferentes variables o predictores (X).\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+\\dots+\\beta_p X_p$$\n",
    "\n",
    "El procedimiento de ajuste implica una función de pérdida, conocida como suma residual de cuadrados o RSS, y los coeficientes se eligen de manera que minimizan esta función de pérdida.\n",
    "\n",
    "Ahora, esto ajustará los coeficientes en función de sus datos de entrenamiento. Si hay ruido en los datos de entrenamiento, entonces los coeficientes estimados no se generalizarán bien a los datos futuros. Aquí es donde entra la regularización y reduce o regulariza estas estimaciones aprendidas hacia cero\n",
    "\n",
    "$$ RSS = \\displaystyle\\sum_{i=1}^{n} \\big(y_i - \\beta_0 - \\sum_{j=1}^{p}B_jx_{ij}\\big)^2$$\n",
    "\n",
    "Algunos métodos comúnes que apliquen esta técnica son Ridge y Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son los componentes de las técnicas de evaluación relacional?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta**\n",
    "* Data Acquisition\n",
    "* Ground Truth Acquisition\n",
    "* Cross-Validation Technique\n",
    "* Query Type\n",
    "* Scoring Metric\n",
    "* Significance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define el F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "En análisis estadístico de clasificación binaria, el F1 score es una medida de la exactitud\n",
    "\n",
    "$$F_1 =  2\\frac{p.s}{p+s}$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* p: presición $\\frac{TP}{TP+FP}$\n",
    "\n",
    "* s: sensibilidad $\\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo seleccionas variables importantes en un conjunto de datos?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:** Yo haría lo siguiente\n",
    "* Eliminar las variables correlacionadas antes de seleccionar variables importantes\n",
    "* Medir la ganancia de información para el conjunto de características disponibles y seleccione las mejores características (en pocas palabras, generar árboles de desición, mediante RandomForest por ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es un sistema de recomendación?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Usualmente es el producto final de un modelo entrenado para clasificar algún tópico de interés a partir de data del mismo tópico, así se puede recomendar un item a partir de los items previamente consumidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo combatir la maldición de la dimensionalidad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Si el problema de un modelo de estudio esla dimensionalidad existen dos maneras de manejarlo (almenos son estas dos las que se me ocurren en primera instancia)\n",
    "\n",
    "1. Aplicar un algoritmo de reducción manual de dimension (PCA, Random Projections, etc)\n",
    "2. Discernir entre los *features* útiles, repetidos o redundantes de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explica el análisis de componentes principales (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta**\n",
    "\n",
    "La explicación intuitiva que daría yo sin profundizar en los detalles matemáticos (que ni yo mismo entiendo con exactitud) sería como sigue:\n",
    "\n",
    "Imaginemos que deseamos acotar o *encerrar* el rango de datos de entrada en una elipsoide n-dimensional; es decir, si nuestros datos tuviesen 3 *features* tendríamos una elipse en $R^3$, con cada eje representando una componente principal. Ahora bien, supongamos que la varianza de una de las características es baja, entonces uno de los ejes de la elipsoide será pequeño y podríamos decir que, geométricamente, no nos dota de volumen (si seguimos hablando en $R^3$); por lo que deshacernos de este eje y utilizando como datos solo los contenidos en este nuevo disco elíptico ahora en $R^2$ no alteraría mucho nuestro trabajo. Trasladando esta idea al algebra lineal tendremos un elipsoide n-dimensional del cual proyectaremos los ejes mediante un cambio de base, descartando los de menor rango (o menor varianza) y utilizando el resultado para generar nuestros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es la normalización de datos y por qué la necesitamos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De mi poca experiencia con sistemas de datos numéricos puedo decir que muchas veces la normalización de datos funciona como una **buena práctica obligatoria**, cosa que no tendría sentido sin contexto:\n",
    "\n",
    "La normalización de datos es el proceso por el cual se utiliza un estándar predefinido para transformar los datos a una forma trabajable de manera óptima por el estándar. Muchas veces este estándar de normalización se confunde con la normalización numérica, y con razón dado que esta práctica es la mas utilizada, pero no estrictamente obligatoria. En resumen: la normalización (de datos o numérica) consta de cambiar los valores a un rango predefinido de trabajo; dado que estos rangos son los que demuestran mejor performance en sistemas de ML.\n",
    "\n",
    "¿Por qué las necesitamos? En realidad no las necesitamos, porque la mayoría de veces se llega al resultado teniendo el sistema normalizado o no, el problema surge cuando el resultado es inútil (no deja de ser un modelo funcional, que sea útil o no depende del investigador).\n",
    "\n",
    "El problema, sobretodo en modelos de regresión, es la función de costo que, si expulsa datos \"normalizados\", compararía matemáticamente algo de la forma:\n",
    "\n",
    "$$ J(\\theta) = f(|\\hat y - y|) $$\n",
    "\n",
    "Donde el $\\hat y$ está volando con valores muy grandes en comparación al $y$ esperado que se supone es normalizado; por lo que se resuelve normalizando el input y así disminuyendo el riesgo de no-convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo manejas los datos faltantes en un conjunto de datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es que usar algún método de reducción de dimension está fuera de las posibilidades haría lo siguiente:\n",
    "* Si los datos son categóricos eliminaría las entradas con datos faltantes y seguiría mi camino\n",
    "* Si los datos fuesen numéricos haría varías pruebas:\n",
    "    * Rellenar los faltantes con la media de los datos presentes y evaluar las métricas\n",
    "    * Eliminar las entradas faltantes y evaluar la métricas\n",
    "    * Separar los datos con entradas faltantes, evaluar metricas con los restantes e intentar predecir los faltantes a partir del modelo. Luego generar un segundo modelo con los datos predecidos como input original y revisar qué tan lejos se está del primer modelo generado\n",
    "   \n",
    "Luego de hacer esto basaría mis conclusiones en comparar las métricas obtenidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo harías para hacer un análisis exploratorio de datos (EDA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta:**\n",
    "Generalmente la tarea de exploración de datos está ligado a las gráficas obtenidas a partir de los mismos. Yo realizaría dicha exploración ploteando histogramas y gráficos de dispersión para revisar la distribución de los datos en el contexto estadístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué necesitamos un conjunto de validación y un conjunto de prueba? ¿Cuál es la diferencia entre ellos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta**\n",
    "Se utiliza el conjunto de validación para validar el modelo recién generado a partir de tus datos de entrenamiento, mientras que el conjunto de prueba tiende a ser data real de la cual no se sabe el output aún, y que se espera estimar usando el modelo validado recién. Uno da confianza al modelo, el otro da miedo al desarrollador. Uno corrobora tus resultados, el otro genera resultados antes desconocidos. Uno sirve para revisar métricas y probar performance, el otro sirve revisar tu confianza y probar tu ansiedad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es la validación cruzada estratificada y cuándo debemos usarla?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta.**\n",
    "La estratificación es el proceso de reorganizar los datos para garantizar que cada pliegue sea un buen representante del conjunto. Por ejemplo, en un problema de clasificación binaria donde cada clase comprende el 50% de los datos, es mejor organizar los datos de manera que en cada pliegue, cada clase comprenda alrededor de la mitad de los casos.\n",
    "\n",
    "La estratificación es generalmente un esquema mejor, tanto en términos de sesgo como de varianza, en comparación con la validación cruzada regular; esto puede ser útil cuando tenemos datos muestrales limitados donde tendríamos diferencias extremistas en la distribución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué los modelos conjuntos o ensamblados suelen tener puntuaciones más altas que los modelos individuales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta no encontrada**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es un conjunto de datos desbalanceado? ¿Puedes enumerar algunas maneras de lidiar con estos conjuntos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rpta.**\n",
    "Un conjunto de datos está desbalanceado si las clases minoritarias están sub-representadas en oposición a las clases mayoritarias; es decir, no existe la suficiente cantidad de datos como para representar un modelo confiable que clasifique elementos en las clases minorotarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "* https://pdfs.semanticscholar.org/4e50/101c7650771fbdbaec8d85e7858f088d28e3.pdf\n",
    "* https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set\n",
    "* https://studylib.net/doc/5365737/relational-evaluation-techniques---graph-rat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
